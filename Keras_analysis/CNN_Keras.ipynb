{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-1bd1c2ef0873>:54: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#TF Librairies Import \n",
    "from tensorflow.keras.metrics import TopKCategoricalAccuracy\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D, Conv2D, MaxPooling2D, AveragePooling2D, Input, GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.optimizers.experimental import AdamW, Optimizer\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as pi_16\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input as pi_19\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as pi_densenet\n",
    "from tensorflow.keras.applications.xception import preprocess_input as pi_xception\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input as pi_mobilnet\n",
    "from tensorflow.keras.applications import Xception, MobileNetV2, MobileNetV3Large, DenseNet201\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow_addons.optimizers import CyclicalLearningRate\n",
    "import keras\n",
    "\n",
    "#Traditionnals librairies import\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score,\n",
    "from useful_functions import create_tensorboard_callback\n",
    "import datetime\n",
    "from timeit import default_timer as timer\n",
    "import tensorflow as tf\n",
    "from PIL import Image \n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time, cv2\n",
    "import seaborn as sns\n",
    "import math\n",
    "import glob\n",
    "import absl.logging\n",
    "\n",
    "#Display parameters\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation des paramètres et du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VARIABLE CREATION ##\n",
    "\n",
    "#Global\n",
    "top = 100\n",
    "number_images_selected = 300\n",
    "rank = 'species'\n",
    "\n",
    "#Paths\n",
    "origine_picture = 'gbif'\n",
    "workdir = '/Users/cgm_mbf_lemery_mac/Documents/DataScience/mushroom_dataset/'\n",
    "path_LR_determiner = workdir + 'LR_determiner/'\n",
    "path_models = workdir + 'models/'\n",
    "path_graphes = workdir + 'graphes/'\n",
    "path_images = workdir +  'images/'\n",
    "path_cfn = workdir +  'confusion_matrix/'\n",
    "dir_logs = workdir + 'tensorboard_logs/'\n",
    "path_history = workdir + 'history/'\n",
    "data_images = workdir + 'data_images/'\n",
    "train_directory = data_images + 'images_'+str(origine_picture)+'_'+str(top)+'_'+str(number_images_selected)+'/train/'\n",
    "test_directory = data_images + 'images_'+str(origine_picture)+'_'+str(top)+'_'+str(number_images_selected)+'/test/'\n",
    "validation_directory = data_images + 'images_'+str(origine_picture)+'_'+str(top)+'_'+str(number_images_selected)+'/val/'\n",
    "\n",
    "\n",
    "#Callback Parameters\n",
    "patience_learning_rate_tuning= 1                    #Time to wait to get learning rate adaptation callback - Tuning\n",
    "patience_learning_rate_fine_tuning = 1              #Time to wait to get learning rate adaptation callback - Fine Tuning\n",
    "cooldown_learning_rate_tuning =0                    #Time to wait wait after learning rate adaptation callback - Tuning\n",
    "cooldown_learning_rate_fine_tuning = 0              #Time to wait wait after learning rate adaptation callback - Fine Tuning\n",
    "reduce_learning_rate_type = 'ReduceLROnPlateau'     #Just for naming\n",
    "factor_learning_rate_reducing_initial = 0.90        #Reduction factor for learning rate - Tuning\n",
    "factor_learning_rate_reducing_tuning = 0.90         #Reduction factor for learning rate - Fine Tuning\n",
    "min_delta_rlr = 0.02                                #Minimal delta value of variation to activate learning rate reduction\n",
    "min_delta_stop = 0.01                               #Minimal delta value before to stop training\n",
    "patience = 4                                        #Time to wait with no delta stop improvement to stop training                                    \n",
    "\n",
    "#Models Parameters\n",
    "selected_model = 'vgg19'                            #Available : VGG16, VGG19, Xception, MobilNetV2, densenet201\n",
    "top_k_accuracy = 3                                  #Analyse the accuracy that the prediction is in the k first results\n",
    "layer_dense = 1                                     #Number of dense layer\n",
    "epoch_tuning = 45                                   #Number of epoch for the raw period\n",
    "epoch_fine_tuning = 20                              #Epoch for Fine Tuning\n",
    "number_phase_fine_tuning = 0                        #Si aucun tuning : 0\n",
    "dropout = 0.5                                       #Dropout rate\n",
    "batch_size = 80                                     #Batch size\n",
    "target_size = (224,224)                             #Size of image in the batches\n",
    "batch_normalization = 'Yes'                         #Is there BN in the classificator\n",
    "\n",
    "#Lr finder parameter\n",
    "start_lr = 1e-06                                    #Start Lr for LR Finder analysis\n",
    "end_lr = 0.1                                        #End Lr for LR Finder analysis\n",
    "epoch_lr_determiner = 20                            #Number of epoch to run the LR Finder analyse. Allow to control the number of tested value. Lr is changed every 3 batches\n",
    "\n",
    "#Regularization\n",
    "kernel_regularizer_value = 1e-02                    #Kernel regularization value in the classificator (if L1L2, then the value is L2 and L1 = L2/10)\n",
    "kernel_regularizer_type = None                      #None, L1, L2, L1L2\n",
    "\n",
    "#Optimizer\n",
    "learning_rate_tuning = 3e-03\n",
    "learning_rate_fine_tuning = 2e-05                   #Learning rate for Fine Tuning\n",
    "weight_decay = 5e-05                                #Weight Decay                     \n",
    "optimizer_type = 'RMSprop'                          #SGD, Adam et AdamW\n",
    "loss = 'categorical_crossentropy'                   #Loss function\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object creation for training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part concerns the creation of image generators. They allow to apply data augmentation in order to fight against overfitting\n",
    "The index validation split allows to specify that this generator can generate two files of augmented images: One for test and one for validation\n",
    "\"\"\"\n",
    "\n",
    "selected_model = selected_model.lower()\n",
    "dictionnaire_pi = {'vgg16' : pi_16,'vgg19' : pi_19, 'xception' : pi_xception, 'mobilnetv2' : pi_mobilnet, 'densenet' : pi_densenet, }\n",
    "\n",
    "train_data_generator = ImageDataGenerator(preprocessing_function= dictionnaire_pi[selected_model],\n",
    "                                          rotation_range= 45,\n",
    "                                          width_shift_range= 0.22,\n",
    "                                          height_shift_range = 0.22,\n",
    "                                          zoom_range = 0.22,\n",
    "                                          shear_range= 0.22,\n",
    "                                          fill_mode='nearest',\n",
    "                                          horizontal_flip=True,\n",
    "                                          vertical_flip=True,\n",
    "                                          #brightness_range=(0.3, 0.9), \n",
    "                                          validation_split = 0.2)\n",
    "test_data_generator = ImageDataGenerator(preprocessing_function = dictionnaire_pi[selected_model])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part concerns the creation of sets of augmented images. \n",
    "It allows you to create from the 'train' and 'test' image folders, the train, validation and test image sets\n",
    "\"\"\"\n",
    "\n",
    "train_generator = train_data_generator.flow_from_directory(directory = train_directory, \n",
    "                                                            class_mode = 'categorical', target_size = target_size, batch_size = batch_size,subset = 'training', color_mode = 'rgb', seed = 42, shuffle = True )\n",
    "\n",
    "validation_generator = train_data_generator.flow_from_directory(directory = train_directory, \n",
    "                                                            class_mode = 'categorical', target_size = target_size, batch_size = batch_size, subset = 'validation', color_mode = 'rgb', seed = 42, shuffle = True)\n",
    "\n",
    "test_generator = test_data_generator.flow_from_directory(directory = test_directory, \n",
    "                                                            class_mode = 'categorical', target_size = target_size, batch_size = batch_size, color_mode = 'rgb', seed = 42, shuffle = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition des fonctions base_model Kernel_choice and add_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part concerns the creation of functions allowing the creation of models, optimizers and penalties\n",
    "\"\"\"\n",
    "\n",
    "def select_base_model(selected_model):\n",
    "    \"\"\"\n",
    "    Base model selection\n",
    "    \"\"\"\n",
    "    selected_model = selected_model.lower()\n",
    "    if selected_model == 'vgg16':\n",
    "        base_model = VGG16(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\n",
    "        return base_model\n",
    "    elif selected_model == 'vgg19':\n",
    "        base_model = VGG19(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\n",
    "        return base_model\n",
    "    elif selected_model == 'xception':\n",
    "        base_model = Xception(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3), pooling='avg')\n",
    "        return base_model\n",
    "    elif selected_model == 'mobilnetv2':\n",
    "        base_model = MobileNetV2(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3), pooling='avg')\n",
    "        return base_model\n",
    "    elif selected_model == 'densenet':\n",
    "        base_model = DenseNet201(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3), pooling='avg')\n",
    "        return base_model\n",
    "    else :\n",
    "        raise ValueError('Select model : vgg16, vgg19, Xception or MobilNetV2')\n",
    "\n",
    "\n",
    "def kernel_choice(kernel_regularizer_type, kernel_regularizer_value):\n",
    "    \"\"\"\n",
    "    Penalty selection\n",
    "    \"\"\"\n",
    "    if kernel_regularizer_type == 'l1':\n",
    "        kernel_regularizer = regularizers.L1(kernel_regularizer_value)\n",
    "        return kernel_regularizer\n",
    "    elif kernel_regularizer_type == 'l2' :\n",
    "        kernel_regularizer = regularizers.L2(kernel_regularizer_value)\n",
    "        return kernel_regularizer\n",
    "    elif kernel_regularizer_type == 'l1l2' :\n",
    "        kernel_regularizer = regularizers.L1L2(l1 = kernel_regularizer_value, l2 = kernel_regularizer_value)\n",
    "        return kernel_regularizer\n",
    "    elif kernel_regularizer_type == None :\n",
    "        print('Aucune regularization kernel')\n",
    "    else : raise ValueError('Define kernel regularization')\n",
    "\n",
    "def construction_model(base_model, selected_model, kernel_regularizer_type, kernel_regularizer_value):\n",
    "    \"\"\"\n",
    "    Model construction\n",
    "    \"\"\"\n",
    "    selected_model = selected_model.lower()\n",
    "    if selected_model == 'vgg16' or selected_model == 'vgg19':\n",
    "        model = Sequential()\n",
    "        model.add(base_model)\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "        model = add_layer(layer_dense, kernel_regularizer_type, model, kernel_regularizer_value)\n",
    "        return model\n",
    "    if selected_model == 'xception' or selected_model == 'mobilnetv2' or selected_model == 'densenet' :\n",
    "        model = Sequential()\n",
    "        model.add(base_model)\n",
    "        model = add_layer(layer_dense, kernel_regularizer_type, model, kernel_regularizer_value)\n",
    "        return model\n",
    "\n",
    "def add_layer(layer_dense, kernel_regularizer_type, model, kernel_regularizer_value) :\n",
    "    \"\"\"\n",
    "    Classificator construction\n",
    "    \"\"\"\n",
    "    if kernel_regularizer_type == None :\n",
    "        if layer_dense == 1:\n",
    "            model.add(Dense(units = 1024, activation = 'relu'))\n",
    "            if batch_normalization.lower() == 'yes':\n",
    "                model.add(BatchNormalization())\n",
    "            model.add(Dropout(rate =dropout))\n",
    "            model.add(Dense(units = top, activation = 'softmax'))\n",
    "            return model\n",
    "\n",
    "        elif layer_dense == 2:\n",
    "            model.add(Dense(units = 1024, activation = 'relu'))\n",
    "            if batch_normalization.lower() == 'yes':\n",
    "                model.add(BatchNormalization())\n",
    "            model.add(Dropout(rate =dropout))\n",
    "            model.add(Dense(units = 512, activation = 'relu'))\n",
    "            if batch_normalization.lower() == 'yes':\n",
    "                model.add(BatchNormalization())\n",
    "            model.add(Dropout(rate = dropout))\n",
    "            model.add(Dense(units = top, activation = 'softmax'))\n",
    "            return model\n",
    "\n",
    "        elif layer_dense == 3:\n",
    "            model.add(Dense(units = 1024, activation = 'relu'))\n",
    "            model.add(Dropout(rate =dropout))\n",
    "            if batch_normalization.lower() == 'yes':\n",
    "                model.add(BatchNormalization())\n",
    "            model.add(Dense(units = 512, activation = 'relu'))\n",
    "            model.add(Dropout(rate = dropout))\n",
    "            if batch_normalization.lower() == 'yes':\n",
    "                model.add(BatchNormalization())\n",
    "            model.add(Dense(units = 254, activation = 'relu'))\n",
    "            model.add(Dropout(rate = dropout))\n",
    "            if batch_normalization.lower() == 'yes':\n",
    "                model.add(BatchNormalization())\n",
    "            model.add(Dense(units = top, activation = 'softmax'))\n",
    "            return model\n",
    "    \n",
    "    elif kernel_regularizer_type == 'l1' or kernel_regularizer_type == 'l2' or kernel_regularizer_type == 'l1l2' :\n",
    "        if layer_dense == 1:\n",
    "            model.add(Dense(units = 1024, activation = 'relu', kernel_regularizer = kernel_choice(kernel_regularizer_type,kernel_regularizer_value)))\n",
    "            model.add(Dropout(rate =dropout))\n",
    "            if batch_normalization.lower() == 'yes':\n",
    "                model.add(BatchNormalization())\n",
    "            model.add(Dense(units = top, activation = 'softmax'))\n",
    "            return model\n",
    "\n",
    "        elif layer_dense == 2:\n",
    "            model.add(Dense(units = 1024, activation = 'relu', kernel_regularizer = kernel_choice(kernel_regularizer_type, kernel_regularizer_value)))\n",
    "            model.add(Dropout(rate =dropout))\n",
    "            if batch_normalization.lower() == 'yes':\n",
    "                model.add(BatchNormalization())\n",
    "            model.add(Dense(units = 512, activation = 'relu', kernel_regularizer = kernel_choice(kernel_regularizer_type, kernel_regularizer_value)))\n",
    "            model.add(Dropout(rate = dropout))\n",
    "            if batch_normalization.lower() == 'yes':\n",
    "                model.add(BatchNormalization())\n",
    "            model.add(Dense(units = top, activation = 'softmax'))\n",
    "            return model\n",
    "\n",
    "        elif layer_dense == 3:\n",
    "            model.add(Dense(units = 1024, activation = 'relu', kernel_regularizer = kernel_choice(kernel_regularizer_type, kernel_regularizer_value)))\n",
    "            model.add(Dropout(rate =dropout))\n",
    "            if batch_normalization.lower() == 'yes':\n",
    "                model.add(BatchNormalization())\n",
    "            model.add(Dense(units = 512, activation = 'relu', kernel_regularizer = kernel_choice(kernel_regularizer_type, kernel_regularizer_value)))\n",
    "            model.add(Dropout(rate = dropout))\n",
    "            if batch_normalization.lower() == 'yes':\n",
    "                model.add(BatchNormalization())\n",
    "            model.add(Dense(units = 254, activation = 'relu', kernel_regularizer = kernel_choice(kernel_regularizer_type, kernel_regularizer_value)))\n",
    "            model.add(Dropout(rate = dropout))\n",
    "            if batch_normalization.lower() == 'yes':\n",
    "                model.add(BatchNormalization())\n",
    "            model.add(Dense(units = top, activation = 'softmax'))\n",
    "            return model\n",
    "    else :\n",
    "        raise ValueError('Issue with regularizer : is not l1, or l2, or l1l2, or None OR Define a integer between 1 and 3 for the number of dense layers')\n",
    "\n",
    "def list_split(nnumber_phase_tuning,number_of_layer) :\n",
    "    \"\"\"\n",
    "    List creation for Tuning / Fine-Tuning iteration\n",
    "    \"\"\"\n",
    "    if number_phase_tuning == 0 : \n",
    "        list_splits = [0]\n",
    "        return list_splits\n",
    "    elif  number_phase_tuning != 0 :\n",
    "        list_splits = [round((number_of_layer/number_phase_tuning)*i) for i in np.arange(0,number_phase_tuning+1)]\n",
    "        return list_splits\n",
    "    else :\n",
    "        raise ValueError('Issue with number_splits')\n",
    "\n",
    "\n",
    "def optimizer_choice(optimizer_type, learning_rate_params):\n",
    "    \"\"\"\n",
    "    Selection des optimizers. \n",
    "    Tout nouvel optimizer doit être ajouté ici.\n",
    "    Attention aux paramètres d'entrée de la fonction si de nouveaux arguments\n",
    "    \"\"\"\n",
    "    optimizer_type = optimizer_type.lower()\n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = Adam(learning_rate = learning_rate_params)\n",
    "        return optimizer\n",
    "    elif optimizer_type == 'adamw':\n",
    "        optimizer = AdamW(learning_rate= learning_rate_params,  beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, clipnorm=None,\n",
    "        clipvalue=None, global_clipnorm=None, use_ema=False, ema_momentum=0.99, ema_overwrite_frequency=None, jit_compile=False, name=\"AdamW\")\n",
    "        return optimizer\n",
    "    elif optimizer_type == 'adamcdr':\n",
    "        lr_decayed = CosineDecayRestarts(initial_learning_rate = learning_rate_params, weight_decay = weight_decay, alpha = 0.01)\n",
    "        optimizer = Adam(learning_rate= lr_decayed)\n",
    "        return optimizer\n",
    "    elif optimizer_type == 'sgd':\n",
    "        optimizer = SGD(learning_rate =  learning_rate_params, weight_decay = weight_decay)\n",
    "        return optimizer\n",
    "    elif optimizer_type == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate =  learning_rate_params)\n",
    "        return optimizer\n",
    "    else : raise ValueError('Optimizer non enregistré')\n",
    "\n",
    "#Learning rate reduction callback\n",
    "def step_decay(epoch, lr):\n",
    "    drop = 0.98\n",
    "    epochs_drop = 1\n",
    "    return  lr * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation\n",
    "\n",
    "## Select just one of the two following method : Sequential or Functionnal. Functionnal is not automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SEQUENTIAL MODEL CREATION ##\n",
    "\n",
    "#Model de base et model total\n",
    "base_model = select_base_model(selected_model)\n",
    "model = construction_model(base_model, selected_model, kernel_regularizer_type, kernel_regularizer_value)\n",
    "\n",
    "#Model de base et model total\n",
    "number_of_layer = len(base_model.layers)\n",
    "list_splits =list_split(number_phase_fine_tuning,number_of_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FONCTIONNAL MODEL CREATION ##\n",
    "\n",
    "dense_1_units = 1024\n",
    "dense_2_units = 512\n",
    "dense_3_units = 256\n",
    "\n",
    "#Model creation\n",
    "dense1 = Dense(units = dense_1_units, activation = 'relu', kernel_initializer = 'normal', name = 'Dense_1')\n",
    "dense2 = Dense(units = dense_2_units, activation = 'relu', kernel_initializer = 'normal', name = 'Dense_2')\n",
    "dense3 = Dense(units = dense_3_units, activation = 'relu', kernel_initializer = 'normal', name = 'Dense_3')\n",
    "\n",
    "max_pooling = GlobalMaxPooling2D(data_format=None, keepdims=False)\n",
    "avg_pooling = GlobalAveragePooling2D(data_format=None, keepdims=False)\n",
    "dense_softmax = Dense(units = top, activation = 'softmax', kernel_initializer = 'normal', name = 'Dense_softmax')\n",
    "dropout_layer = Dropout(dropout, noise_shape=None, seed=None)\n",
    "bn_layer = BatchNormalization()\n",
    "\n",
    "\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "base_model = DenseNet201(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\n",
    "x = base_model(inputs)\n",
    "x2_prim = avg_pooling(x)\n",
    "x2_bis = max_pooling(x)\n",
    "x2 = tf.keras.layers.concatenate([x2_prim, x2_bis])\n",
    "x3 = dense1(x2)\n",
    "x4 = dropout_layer(x3)\n",
    "x5 = bn_layer(x4)\n",
    "x6 = dense_softmax(x5)\n",
    "Outputs = x6\n",
    "model = Model(inputs = inputs, outputs = Outputs)\n",
    "\n",
    "#Split list creation\n",
    "number_of_layer = len(base_model.layers)\n",
    "list_splits =list_split(number_phase_fine_tuning,number_of_layer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naming and folder creation\n",
    "name_history = (str(selected_model)+'_'+str(top)+'_'+str(number_images_selected)+'_'+str(rank)+str(reduce_learning_rate_type)+'_'+'LR_raw-'+str(\"%.1e\" % learning_rate_raw_initial)+'_pat_lr_init-'+str(patience_learning_rate_initial)+'_coold_lr_init-'+str(cooldown_learning_rate_initial)+'_deltaRLR-'+str(min_delta_rlr)+'_optim-'+str(optimizer_type)+'_epochs_init-'+str(epoch_raw)+'_'+'DO-'+str(dropout)+'_'+'layer_dense-'+str(layer_dense)+'_'+'kernel_reg-'+str(kernel_regularizer_type)+'-'+str(kernel_regularizer_value)+'_'+origine_picture+'_'+'BatchNorm-'+str(batch_normalization))\n",
    "directory_name = str(datetime.datetime.now()).replace(\":\", '_').replace(\" \", '_')\n",
    "path_history_deep = os.path.join(path_history, directory_name + '_' + name_history)\n",
    "path_graphes_deep = os.path.join(path_graphes, directory_name + '_' + name_history)\n",
    "path_models_deep = os.path.join(path_models, directory_name + '_' + name_history)\n",
    "os.mkdir(path_history_deep)\n",
    "os.mkdir(path_graphes_deep)\n",
    "os.mkdir(path_models_deep)\n",
    "\n",
    "#List creation for Tuning and Fine-Tuning iterations\n",
    "name_params = []\n",
    "path_model_params = []\n",
    "path_graphs_params = []\n",
    "learning_rate_params = []\n",
    "factor_learning_rate_reducing = []\n",
    "patience_learning_rate_params = []\n",
    "cooldown_learning_rate_params = []\n",
    "epochs =[]\n",
    "\n",
    "#Dataframe for history concatenation\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#Model training loop: We go through the list of plit to train the classifier, then the CNN layers by tuning\n",
    "#The parameters are identical between the tuning phases, except for the learning rate\n",
    "\n",
    "for index, split in enumerate(list_splits) :\n",
    "\n",
    "    #Définition of variables for loops\n",
    "    if index == 0: #RAW\n",
    "        learning_rate_params.append(learning_rate_tuning)\n",
    "        factor_learning_rate_reducing.append(factor_learning_rate_reducing_initial)\n",
    "        patience_learning_rate_params.append(patience_learning_rate_tuning)\n",
    "        cooldown_learning_rate_params.append(cooldown_learning_rate_tuning)\n",
    "        epochs.append(epoch_tuning)\n",
    "    else :  #TUNING\n",
    "        learning_rate_params.append(learning_rate_fine_tuning/(1.2**(index)))\n",
    "        factor_learning_rate_reducing.append(factor_learning_rate_reducing_tuning)\n",
    "        patience_learning_rate_params.append(patience_learning_rate_fine_tuning)\n",
    "        cooldown_learning_rate_params.append(cooldown_learning_rate_fine_tuning)\n",
    "        epochs.append(epoch_fine_tuning)\n",
    "\n",
    "\n",
    "    #Définition des noms pour les enregistrement (on sauvegarde les fichiers par phases)\n",
    "    name_params.append(str(top)+'_'+str(number_images_selected)+'_'+str(rank)+'_split_num-'+str(index)+'_RLR_policy-'+str(reduce_learning_rate_type)+'_'+'flr_reduc_ini-'+str(\"%.1e\" % learning_rate_params[index])+'_pat_lr-'+str(patience_learning_rate_params[index])+'_coold_lr_'+str(cooldown_learning_rate_params[index])+'_deltaRLR-'+str(min_delta_rlr)+'_optim-'+str(optimizer_type)+'_epochs-'+str(epochs[index])+'_'+'DO-'+str(dropout)+'_'+'layer_dense-'+str(layer_dense)+'_'+'kernel_reg-'+str(kernel_regularizer_type)+'-'+str(kernel_regularizer_value)+'_'+origine_picture+'_BatchNorm-'+str(batch_normalization))\n",
    "    path_model_params.append(path_models_deep+'/'+(selected_model)+'_'+str(top)+'_'+str(number_images_selected)+'_'+str(rank)+'_split_num-'+str(index)+'_RLR_policy-'+str(reduce_learning_rate_type)+'_flr_reduc_init-'+str(\"%.1e\" %  learning_rate_params[index])+'_pat_lr-'+str(patience_learning_rate_params[index])+'_coold_lr_'+str(cooldown_learning_rate_params[index])+'_deltaRLR-'+str(min_delta_rlr)+'_optim-'+str(optimizer_type)+'_epochs-'+str(epochs[index])+'_'+'DO-'+str(dropout)+'layer_dense-'+str(layer_dense)+'_'+'kernel_reg-'+str(kernel_regularizer_type)+'-'+str(kernel_regularizer_value)+'_BatchNorm-'+str(batch_normalization)+'_'+origine_picture+'.h5')\n",
    "    path_graphs_params.append(path_graphes_deep+'/'+(selected_model)+'_'+str(top)+'_'+str(number_images_selected)+'_'+str(rank)+'_split_num-'+str(index)+'_RLR_policy-'+str(reduce_learning_rate_type)+'_flr_reduc_init-'+str(\"%.1e\" % learning_rate_params[index])+'_pat_lr-'+str(patience_learning_rate_params[index])+'_coold_lr_'+str(cooldown_learning_rate_params[index])+'_deltaRLR-'+str(min_delta_rlr)+'_optim-'+str(optimizer_type)+'_epochs-'+str(epochs[index])+'_'+'DO-'+str(dropout)+'_'+'layer_dense-'+str(layer_dense)+'_'+'kernel_reg-'+str(kernel_regularizer_type)+'-'+str(kernel_regularizer_value)+'_BatchNorm-'+str(batch_normalization)+'_'+origine_picture+'.pdf')\n",
    "  \n",
    "    #Optimizer\n",
    "    print(learning_rate_params[index])\n",
    "    optimizer = optimizer_choice(optimizer_type, learning_rate_params[index])\n",
    "\n",
    "    #Callbacks\n",
    "    reduce_learning_rate = ReduceLROnPlateau(monitor = 'val_loss', factor = factor_learning_rate_reducing[index], patience = patience_learning_rate_params[index], cooldown = cooldown_learning_rate_params[index], verbose = 1, min_delta=min_delta_rlr)\n",
    "    earlystop = EarlyStopping(monitor = 'val_loss', min_delta = min_delta_stop, patience = patience, verbose = 1, restore_best_weights = True)\n",
    "    LRscheduler = LearningRateScheduler(step_decay)\n",
    "    model_check = ModelCheckpoint(filepath = path_model_params[index], monitor = 'val_loss', mode = 'min')\n",
    "    tensorboard = create_tensorboard_callback(dir_logs, name_params[index])\n",
    "       \n",
    "    if index == 0: #Tuning\n",
    "       #Trainable layers\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    else : #Fine-tuning\n",
    "        for layer in base_model.layers[-split:]:\n",
    "            layer.trainable = True \n",
    "\n",
    "\n",
    "    #Model training\n",
    "    print('Iteration : {}'.format(index), '\\n')\n",
    "    print(model.summary())\n",
    "    model.compile(optimizer = optimizer, loss = loss, metrics = ['acc', TopKCategoricalAccuracy(top_k_accuracy)])\n",
    "    history = model.fit(train_generator, epochs = epochs[index], verbose = 1, validation_data = validation_generator, \n",
    "                        callbacks = [model_check, reduce_learning_rate, tensorboard, earlystop, LRscheduler])\n",
    "    \n",
    "    #History backup\n",
    "    pd.DataFrame.from_dict(history.history).to_csv(path_history_deep + '/' + name_params[index] ,index=False)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame.from_dict(history.history)], axis = 0)\n",
    "    df.to_csv(path_history_deep + '/' + name_history ,index=False)\n",
    "    \n",
    "\n",
    "    #Plots\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history[\"acc\"]\n",
    "    val_acc = history.history[\"val_acc\"]\n",
    "    fig = plt.figure(figsize = (20, 12))\n",
    "    fig = plt.subplot(211)\n",
    "    fig = plt.plot(train_loss)\n",
    "    fig = plt.plot(val_loss)\n",
    "    fig = plt.title('Model Cross_entropy : ' + name_params[index])\n",
    "    fig = plt.ylabel('loss')\n",
    "    fig = plt.xlabel('epoch')\n",
    "    fig = plt.legend(['train', 'validation'], loc='right')\n",
    "    fig = plt.subplot(212)\n",
    "    fig = plt.plot(train_acc)\n",
    "    fig = plt.plot(val_acc)\n",
    "    fig = plt.title('Model accuracy per epoch (ACC) : ' + name_params[index])\n",
    "    fig = plt.ylabel('acc')\n",
    "    fig = plt.xlabel('epoch')\n",
    "    fig = plt.legend(['train', 'validation'], loc='right')\n",
    "    plt.savefig(path_graphs_params[index])\n",
    "    fig = plt.show()\n",
    "\n",
    "    #Accuracy on testing data\n",
    "    test_pred = model.predict(test_generator)\n",
    "    test_pred_class = test_pred.argmax(axis = 1)\n",
    "    y_test = test_generator.classes\n",
    "    y_test_one_hot = pd.get_dummies(y_test)\n",
    "    m = tf.keras.metrics.TopKCategoricalAccuracy(k=top_k_accuracy)\n",
    "    m.update_state(y_test_one_hot, test_pred)\n",
    "    result_k = m.result().numpy()\n",
    "    print('The k_accuracy score iteration {} : '.format(index),result_k)\n",
    "    score = accuracy_score(y_test, test_pred_class)\n",
    "    print('The accuracy score iteration {} : '.format(index), score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Finder Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class to calcul the best Lr according to Smith method. Modified from Github.\n",
    "\"\"\"\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    def __init__(self, min_lr, max_lr, path, name, mom=0.9, stop_multiplier=None, \n",
    "                 reload_weights=True, batches_lr_update=2):\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.name = name\n",
    "        self.mom = mom\n",
    "        self.path = path\n",
    "        self.reload_weights = reload_weights\n",
    "        self.batches_lr_update = batches_lr_update\n",
    "        if stop_multiplier is None:\n",
    "            self.stop_multiplier = -20*self.mom/3 + 10 # 4 if mom=0.9\n",
    "                                                       # 10 if mom=0\n",
    "        else:\n",
    "            self.stop_multiplier = stop_multiplier\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        p = self.params\n",
    "        print(p)\n",
    "        try:\n",
    "            n_iterations = p['epochs']*p['samples']//p['batch_size']\n",
    "        except:\n",
    "            n_iterations = p['steps']*p['epochs']\n",
    "            \n",
    "        self.learning_rates = np.geomspace(self.min_lr, self.max_lr, \\\n",
    "                                           num=n_iterations//self.batches_lr_update+1)\n",
    "        \n",
    "        self.losses=[]\n",
    "        self.iteration=0\n",
    "        self.best_loss=0\n",
    "        if self.reload_weights:\n",
    "            self.model.save_weights('tmp.hdf5')\n",
    "\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        loss = logs.get('loss')\n",
    "        \n",
    "        if self.iteration==0 or loss < self.best_loss: \n",
    "            self.best_loss = loss\n",
    "\n",
    "\n",
    "        if self.iteration!=0: # Make loss smoother using momentum\n",
    "            loss = logs.get('loss')\n",
    "            loss = self.losses[-1]*self.mom+loss*(1-self.mom)\n",
    "                \n",
    "        if self.iteration%self.batches_lr_update==0 :\n",
    "            loss = logs.get('loss')\n",
    "\n",
    "            if self.reload_weights:\n",
    "                self.model.load_weights('tmp.hdf5')\n",
    "          \n",
    "            lr = self.learning_rates[self.iteration//self.batches_lr_update]            \n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "            \n",
    "            self.losses.append(loss)\n",
    "\n",
    "\n",
    "        if loss > self.best_loss*self.stop_multiplier: # Stop criteria\n",
    "            self.model.stop_training = True\n",
    "            self.losses.append(loss)\n",
    "\n",
    "        self.iteration += 1\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.reload_weights:\n",
    "                self.model.load_weights('tmp.hdf5')\n",
    "                \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.learning_rates[:len(self.losses)], self.losses[:])\n",
    "        plt.xlabel(\"Learning Rate\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xscale('log')\n",
    "        plt.show()\n",
    "        df_temp = pd.DataFrame({'Learning_rate' :self.learning_rates[:len(self.losses)], 'Losse' :self.losses[0:]})\n",
    "        df_temp.to_csv(self.path + '/' + self.name ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naming and folder creation\n",
    "name_history = ('LR_DETERMINATION-'+str(selected_model)+'_'+str(top)+'_'+str(number_images_selected)+'_'+str(rank)+'_optim-'+str(optimizer_type)+'_'+'DO-'+str(dropout)+'_'+'layer_dense-'+str(layer_dense)+'_'+origine_picture)\n",
    "directory_name = str(datetime.datetime.now()).replace(\":\", '_').replace(\" \", '_')\n",
    "path_lr_determiner_deep = os.path.join(path_LR_determiner, directory_name + '_' + name_history)\n",
    "os.mkdir(path_lr_determiner_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lr finder training\n",
    "model.compile(loss = loss,metrics=['accuracy'], optimizer=optimizer_type)\n",
    "lr_finder = LRFinder(min_lr=start_lr, max_lr= end_lr, path = path_lr_determiner_deep, name= name_history)\n",
    "model.fit(train_generator, batch_size=50, callbacks=[lr_finder], epochs= epoch_lr_determiner, validation_data = validation_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0aec566b6fd1bcecc66fbc957627a1af46d1c512c44075156b9f706ee0049302"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
